{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Camera pose and orientation estimation from QR code images given real world coordinates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This application visualises the camera position and orientation in the real world. As inputs you need to give in an image whose camera location we need to estimate, the original image and its dimensions. "
   ]
  },
   {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](/multi.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to run the program:\n",
    "\\>>> python pose.py [input_images_dir_path] \\[original_image_path\\] \\[original_imgage_dimension\\]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Searching for the QR code in the given image\n",
    "One assumption made on the basis of the input data is that the input image only has a single QR code lying on an otherwise plain background. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "img_path = '1.jpg'\n",
    "import numpy as np\n",
    "import cv2\n",
    "image = cv2.imread(img_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding Contours\n",
    "First convert the image to greyscale, apply a threshold to get binary image and then apply Canny edge detector. Finally find the contours in the resultant image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_hier_contours(image):\n",
    "    \"\"\"\n",
    "    Returns the contours and their heirarachy for a given image.\n",
    "    \"\"\"\n",
    "    # Constants\n",
    "    thresh = 200\n",
    "    thresh_max = 255\n",
    "    canny_min = 100\n",
    "    canny_max = 200\n",
    "\n",
    "    gray_img = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    th, thresh_img = cv2.threshold(gray_img, thresh, thresh_max, cv2.THRESH_BINARY)\n",
    "    canny = cv2.Canny(thresh_img, canny_min, canny_max)\n",
    "    _, contours, hierarchy = cv2.findContours(canny, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    hierarchy = hierarchy.squeeze()\n",
    "    return hierarchy, contours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hierarchy, contours = get_hier_contours(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](canny.jpg)\n",
    "<center>Canny edge detection.</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting contours that identify the QR code's position markers\n",
    "Loop through all the contours and check each contour for 6 nesting levels to identify the QR code markers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Contour:    \n",
    "    \"\"\"\n",
    "    Represents a contour with its number, the contour data and its centroid.\n",
    "    \"\"\"\n",
    "    def __init__(self, num, contour, centroid):\n",
    "        self.num = num\n",
    "        self.contour = contour\n",
    "        self.centroid = centroid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_centroid(contour):\n",
    "    \"\"\"\n",
    "    Returns the centroid for a given contour.\n",
    "    \"\"\"\n",
    "    m = cv2.moments(contour)\n",
    "    if m['m00'] != 0:\n",
    "        centroid_x =int(m['m10']/m['m00'])\n",
    "        centroid_y =int(m['m01']/m['m00'])\n",
    "    return np.array((centroid_x, centroid_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_position_markers(hierarchy, contours):\n",
    "    \"\"\"\n",
    "    Returns an array of Contour classes for the position markers of the QR code. \n",
    "    \"\"\"\n",
    "    #Constants\n",
    "    nesting_max = 5\n",
    "    conts = []\n",
    "    for h in range(len(hierarchy)):\n",
    "        i = h \n",
    "        nesting = 0\n",
    "        while hierarchy[i][2] != -1:\n",
    "            i = hierarchy[i][2]\n",
    "            nesting += 1\n",
    "        if nesting == nesting_max:\n",
    "            conts.append(Contour(h, contours[h], get_centroid(contours[h])))\n",
    "#     corner_marker_contours = np.asarray((conts[0].contour,conts[1].contour,conts[2].contour))\n",
    "    return np.asarray(conts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conts = get_position_markers(hierarchy, contours)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the relative location of each of the points obtained\n",
    "Going by the conventional structure of a QR code, the bottom right does not have a position marker. So from the 3 points obtained, the 2 points that are equidistant from the third will be opposite corners and the third point will be the top left corner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_dist(point_a, point_b):\n",
    "    \"\"\"\n",
    "    Returns the distance between two 2D coordinates.\n",
    "    \"\"\"\n",
    "    a = point_a.astype(np.float32)\n",
    "    b = point_b.astype(np.float32)\n",
    "    return np.linalg.norm(a - b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vert_opp_largest_side(C1, C2, C3):\n",
    "    \"\"\"\n",
    "    Given 3 points(Contours in this case), returns the 3 contours rearranged\n",
    "    in such a way that the cetroid of first returned contour is opposite to the largest side \n",
    "    in the triangle formed by the three centroids. \n",
    "    \"\"\"\n",
    "    P1 = C1.centroid\n",
    "    P2 = C2.centroid\n",
    "    P3 = C3.centroid\n",
    "    \n",
    "    P1P2 = get_dist(P1, P2)\n",
    "    P2P3 = get_dist(P2, P3)\n",
    "    P3P1 = get_dist(P3, P1)\n",
    "    \n",
    "    if P1P2 > P2P3 and P1P2 > P3P1:\n",
    "        return [C3, C1, C2]\n",
    "    elif P2P3 > P1P2 and P2P3 > P3P1:\n",
    "        return [C1, C2, C3]\n",
    "    elif P3P1 > P1P2 and P3P1 > P2P3:\n",
    "        return [C2, C1, C3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Points representing centroid of the position markers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A, B, C = vert_opp_largest_side(conts[0], conts[1], conts[2])\n",
    "print(A.centroid,B.centroid,C.centroid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get the centroid of the pattern for the 4th point\n",
    "Since OpenCVs API to find extrinsic matrices (translational and rotational) requires a minimum of 4 points, we can select the midpoint of the QR code as the fourth point.Beings a prspective projection, the centroid will be the center of line joiing any two extreme vertices of the square pattern. We have 3 vertices such that, 1 is not a part of the diagonal, so the remaining two can be used to find centroid. (This is an approximation. Since it is a perspective projection, the actual midpoint of the QR code wont lie on the midpoint of the diagonal. But because the subject is not at a very large distance form the camera, this approximation will give considerably good results.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_midpoint(point_a, point_b):\n",
    "    \"\"\"\n",
    "    Returns the midpoint of the line segment joining the given 2 points.\n",
    "    \"\"\"\n",
    "    return np.asarray(((point_a[0]+point_b[0])/2, (point_a[1]+point_b[1])/2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mid = get_midpoint(B.centroid, C.centroid)\n",
    "print(B.centroid,C.centroid,mid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get extreme vertices of position markers\n",
    "The coordinates obtained from the contours A, B and C's represent their centroid. Since we need to get as accurate pose of the camera as possible, we will need to get the extreme corners of these contours of the position markers as that is the measurement we know (8.8cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_max_dist(cnt, point):\n",
    "    \"\"\"\n",
    "    Returns the pointfrom the contour that is farthest from the given point.\n",
    "    \"\"\"\n",
    "    l = np.asarray(cnt[cnt[:,:,0].argmin()][0])\n",
    "    r = np.asarray(cnt[cnt[:,:,0].argmax()][0])\n",
    "    t = np.asarray(cnt[cnt[:,:,1].argmin()][0])\n",
    "    b = np.asarray(cnt[cnt[:,:,1].argmax()][0])\n",
    "    \n",
    "    req = [l,r,t,b]\n",
    "    \n",
    "    max_dist = -1;\n",
    "    max_pts = point\n",
    "    for p in req:\n",
    "        dist = get_dist(p, point)\n",
    "        if max_dist < dist:\n",
    "            max_dist = dist\n",
    "            max_pts = p\n",
    "    return max_pts.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = get_max_dist(A.contour, mid)\n",
    "y = get_max_dist(B.contour, mid)\n",
    "z = get_max_dist(C.contour, mid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mp\n",
    "mp.rcParams['figure.figsize'] = (20,20)\n",
    "\n",
    "points = np.array([A.centroid,B.centroid,C.centroid])\n",
    "corners = np.asarray([x,y,z])\n",
    "corner_marker_contours = np.asarray((conts[0].contour,conts[1].contour,conts[2].contour))\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "cv2.drawContours(image, corner_marker_contours, -1, (0, 255, 0), 3)\n",
    "original = plt.imshow(image)\n",
    "marker_centroids = plt.scatter(points[:, 0], points[:, 1], s = 50)\n",
    "marker_corners = plt.scatter(corners[:,0], corners[:,1], s=50)\n",
    "marker_mid = plt.plot(mid[0], mid[1], 'ro')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](points.jpg)\n",
    "<center>Points located for a sample image.</center>\n",
    "<center><font color=\"#ff0000\">QR centroid(approx)</font> <font color=\"#ffa500\">QR code Coners</font> <font color=\"#00cd00\">Position markers</font> <font color=\"#3232ff\">Position Marker centroid</font> </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Photogammetry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](opencv.jpg)\n",
    "<center> Perspective transfomration. [OpenCV.org](https://docs.opencv.org/2.4/modules/calib3d/doc/camera_calibration_and_3d_reconstruction.html)</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sensor coordinates (u,v)\n",
    "These are obtained from the given image. We assume image coordinates to have origin at the top left corner of the image sensor. These coordinates can be considered as homogeneous coordinates transformed from image coordinate system.  \n",
    " We need to ensure that the ordering of the points is same as the ordering of the points obtained from the image. Here we use the order: top-left, top-right, bottom-left and midpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "image_points = np.array([x, z, y, mid])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Real world coordinates (X,Y,0)\n",
    "The center of the world is considered to be the center of the QR code.  \n",
    "Since we know that that the marker is lying on the floor and is flat, all the real world Z coordinates for the marker will be 0. This reduces a variable from the Perspective transformation.  \n",
    "    We know the actual dimensions of the QR code and that it is a square. So the corners will be at distance actual_dimension/2 from the center."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_world_points(actual_dim):\n",
    "    \"\"\"\n",
    "    Returns the world coordinates of the given the actual dimension of the image, \n",
    "    assuming that the world center is the object center.\n",
    "    \"\"\"\n",
    "    world_points = np.array([(-actual_dim / 2, actual_dim / 2, 0.0),\n",
    "                                 (actual_dim / 2, actual_dim / 2, 0.0),\n",
    "                                 (-actual_dim / 2, -actual_dim / 2, 0.0),\n",
    "                                 (0,0, 0.0)\n",
    "                                 ])\n",
    "    return world_points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Intrinsic Matrix\n",
    "Even though the pinhole camera model does not have a focal length we need it for the scale factor while converting to homgeneous coordinates. The intrinsic camera matrix needs the camera center and the focal length (if we ignore the shear). The camera center can be regarded as the center of the image we have. We can assume the pinhole camera to be a cuboid such that that the distance of the pinhole from the screen (focal length) is equal to either one of the dimensions of the image. Here we consider the height of the image. Another way to determinig the focal length is using camera calibration tool provided in the OpenCV API for even accurate representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_intrinsic(image):\n",
    "    \"\"\"\n",
    "    Returns the intrinsic matrix of the camera based on the above assumptions.\n",
    "    \"\"\"\n",
    "    focal_length = image.shape[0] #px=cm\n",
    "    cam_center_x = image.shape[0] / 2\n",
    "    cam_center_y = image.shape[1] / 2\n",
    "\n",
    "    intrinsic = np.array([[focal_length, 0, cam_center_y],\n",
    "                         [0, focal_length, cam_center_x],\n",
    "                         [0, 0, 1]\n",
    "                         ], dtype=\"double\")\n",
    "    return intrinsic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_cam_vectors(image, image_points, actual_dim, distortion):\n",
    "    \"\"\"\n",
    "    Returns the rotaional and translational vectro of a camera, \n",
    "    given the image, known image point and actual dimensions of the object.\n",
    "    \"\"\"\n",
    "    world_points = get_world_points(actual_dim = 8.8) #cm=px\n",
    "    intrinsic = get_intrinsic(image)\n",
    "    flag, rotation_vector, translation_vector = cv2.solvePnP(world_points,\n",
    "                                                         image_points,\n",
    "                                                         intrinsic,\n",
    "                                                         distortion,\n",
    "                                                         flags=cv2.SOLVEPNP_ITERATIVE)\n",
    "    return rotation_vector, translation_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also assume that there is no distortion. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "distortion = np.zeros((4, 1))\n",
    "actual_dim = 8.8\n",
    "rotation_vector, translation_vector = get_cam_vectors(image, image_points, actual_dim, distortion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert the rotational vector to a rotational matrix (R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rotation_matrix, jacobian = cv2.Rodrigues(rotation_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finding the pose of the camera in the real world system\n",
    "If R is the rotational matrix of the camera, C is the camera pose matrix and T is the translational vector, we will equate RC + T to 0 since we need to find the location of the camera with respect to the world origin (0,0,0). We will leverage the fact that transpose of a rotational matrix is its inverse.\n",
    "\n",
    "=> 0 = RC + T   \n",
    "=> -RC = T  \n",
    "=> Rinv(-RC) = RinvT  \n",
    "=> -C = RtT  \n",
    "=> C = -RtT  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pose = np.matmul(-rotation_matrix.transpose(), translation_vector)\n",
    "pose = pose.squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Camera orientation vector\n",
    "Assuming that the camera is initially looking up the z-axis, [0, 0, 1]T will be the initial orientation vector of the camera.  \n",
    "If Or if the final orientation vector of the camera, then Or can be obtained by transforming [0, 0, 1] using the rotational matrix.\n",
    "\n",
    "=> [0, 0, 1]T = ROr  \n",
    "=> Rinv[0, 0, 1]T = RinvROr  \n",
    "=> Rinv[0, 0, 1]T = Or  \n",
    "=> Rt[0, 0, 1]T = Or  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "orientation = np.matmul(rotation_matrix.T, np.array([0, 0, 1]).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "def get_euler_angles(R):\n",
    "    \"\"\"\n",
    "    Converts the rotational matrix to euler angles in degrees.\n",
    "    (yaw, pitch , roll)\n",
    "    \"\"\"\n",
    "    x = math.atan2(R[2,1] , R[2,2])\n",
    "    y = math.atan2(-R[2,0], math.sqrt(R[0,0] * R[0,0] +  R[1,0] * R[1,0]))\n",
    "    z = math.atan2(R[1,0], R[0,0])\n",
    "    return np.degrees(x), np.degrees(y), np.degrees(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "angles = get_euler_angles(rotation_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_cam(ax, pose, orientation, angles):\n",
    "    \"\"\"\n",
    "    Plotting the camera in real world, given its pose, orientation and angles.\n",
    "    \"\"\"\n",
    "    ax.scatter([pose[0]], [pose[1]], [pose[2]])\n",
    "    label_pose = (\"Pose\", '%.3f' % pose[0], '%.3f' % pose[1], '%.3f' % pose[2])\n",
    "    ax.text(pose[0], pose[1], pose[2], label_pose)\n",
    "    label_orient = (\"Orientation Vector\", '%.3f' % orientation[0], '%.3f' % orientation[1], '%.3f' % orientation[2])\n",
    "    ax.text(pose[0], pose[1], pose[2]+5, label_orient)\n",
    "    label_angle = (\"Yaw,Pitch,Roll\", '%.3f' % angles[0], '%.3f' % angles[1], '%.3f' % angles[2])\n",
    "    ax.text(pose[0], pose[1], pose[2]+10, label_angle)\n",
    "    ax.plot([pose[0], 0],[pose[1], 0], zs=[pose[2], 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import proj3d\n",
    "\n",
    "fig = plt.figure()\n",
    "\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.set_xlabel('X')\n",
    "ax.set_ylabel('Y')\n",
    "ax.set_zlabel('Z')\n",
    "axis_length = max(100, np.amax(pose))\n",
    "ax.set_xlim3d(-axis_length, axis_length)\n",
    "ax.set_ylim3d(-axis_length, axis_length)\n",
    "ax.set_zlim3d(0, axis_length)\n",
    "\n",
    "# Plot camera\n",
    "plot_cam(ax,\n",
    "         np.around(pose, decimals=4),\n",
    "         np.around(orientation, decimals=4),\n",
    "         np.around(angles, decimals=4))\n",
    "\n",
    "# Plot QR Code\n",
    "pattern = cv2.imread('pattern.jpg')\n",
    "pattern = cv2.flip(cv2.transpose(pattern), 0)\n",
    "\n",
    "qr_x, qr_y = np.meshgrid(np.linspace(-actual_dim, actual_dim, pattern.shape[0]),\n",
    "                     np.linspace(-actual_dim, actual_dim, pattern.shape[0]))\n",
    "X = qr_x\n",
    "Y = qr_y\n",
    "Z = 0\n",
    "ax.plot_surface(X, Y, Z, facecolors=pattern / 255., shade=False)\n",
    "\n",
    "plt.gca().set_aspect('equal', adjustable='box')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](result.jpg)\n",
    "<center>Final result</center>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
